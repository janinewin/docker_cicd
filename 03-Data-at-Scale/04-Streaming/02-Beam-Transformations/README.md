# Objectives
<br>

ğŸ¯ The goal is to:
* group data coming from different IoT within a fix interval of time
* load the transformed data in BigQuery

âœ… Before Thinking about streaming the data, we have provided you an old log file which contains values from different sensors with the timestamp. `Streaming` will be covered in next challenge ğŸ™Œ

<br>
<br>

# 1ï¸âƒ£ Download the data
<br>

Let's first download the data that we are going to work with during this challenge.
To do so, let's execute this command:
```bash
mkdir -p data
curl https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/sensors_latency.csv > data/sensors_latency.csv
```
ğŸ‘‰ Let's have a quick look to data in the terminal
```bash
head data/sensors_latency.csv
```
â˜ï¸ The data contains 3 informations:
- The `sensor` name
- The `value` recorded
- The `timeStamp` corresponding to the time of the record
<p>ğŸ’ª Let's build our ETL pipeline with Apache Beam to transform and load this data in BigQuery</p>
<br>
<br>

# 2ï¸âƒ£ Overview of the Pipeline
<br>

During this challenge, We are going to use the paradigm of Map/Reduce with Apache Beam and build step-by-step our pipeline<br>
ğŸ‘‡ Here is an overview of this pipeline

<img src="https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/03-04-02_beam_pipeline.png" witdh=400>


ğŸ‘‰ Head to `app/beam_sample.py` and let's have a look at the different part of the pipeline within the code inside the function `run`

- First a beam Pipeline is instantiated as `pipeline`
- `data`: is the part of the pipeline where data will be loaded
- `windowing`: take as input `data` and will prepare the data to be grouped
- `grouping`: take as input `windowing` then will group and transform the data by key
- `out`: write the output of the pipeline


<br>
<br>

# 3ï¸âƒ£ Read the data with Beam
<br>

ğŸ‘‰ Let's Code the first part of the pipeline in `data` :
- `ReadData` method that will read a text file with the `file_path`variable
- `Convert to list` method that will convert each element into a list


ğŸ’¡ To Debug your code you can map a function to print each element of a PCollection :
```python
data | 'Print in Terminal' >> beam.Map(lambda x: print(x))
```
â˜ï¸ Notice the syntax of Beam to Apply a method with the pipe (|) and name it with >>
<br>
<br>
Now you can execute your code with this command:
```bash
python app/beam_sample.py --input data/sensors_latency.csv
```
<br>
<br>

# 4ï¸âƒ£ Save Data to an outputFile
<br>

Next step is to save the transformed data in an output File.<br>
ğŸ‘‰Head to `out` pipeline and save the data using `output_fps_prefix` variable
<br>

ğŸ’¡ You can apply the method to `grouping` PCollection (For the moment `windowing` and `grouping` are equal to `data` PCollection and they will be implement in the next steps)

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_first_read.txt
```

ğŸ§ªÂ **Test your code**
```bash
make test_first_read
```
<br>
<br>

# 5ï¸âƒ£ Let's Create a key for each element
<br>

Inside `windowing` transformation, Let's create a (key,value) Tuple so it will be possible to group the elements by keys later. <br>
â“In your opinion, what could be a good way to group the data
<details>
    <summary markdown='span'>ğŸ’¡ Hints</summary>

for a TimeStamped data, a good strategy of grouping is to create intervals to gather data from the same time window
</details>
<br>

ğŸ‘‰ Let's code the process by applying a Map method that we name `Map Time Rounded Key per Element`to create a Tuple elements with :
* key: rounded value of the timestamp (You can use `strtime_window_rounded` function)
* value: sensor name and sensor value

â—ï¸We have manually created the time intervals by rounding the timestamp. Actually Beam is able to created time windows with TimeStamped elements -> Let's leave that for the BONUS part.


You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_intermediate_windowing.txt
```

ğŸ‘‰ Let's Head to data/output_intermediate_windowing.txt and have a look at the intermediary output generated by implement windowing transformations.<p>
ğŸ’¡All the first line have the same key with the rounded time, the data is now ready to be grouped into different intervals


ğŸ§ªÂ **Test your code**
```bash
make test_windowing
```
<br>
<br>

# 6ï¸âƒ£ Let's Group/Combine the data
<br>

First we need to group the data. In The previous step, we have already created a key for each element.
So now, we just need to group these elements toghether. Let's call this transformation `Group By Key`

<details>
    <summary markdown='span'>ğŸ’¡ Hints</summary>
You can have a look at the Apache Beam `GroupByKey` Documentation <a href="https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey/">here</a>
</details>
<br>

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_intermediate_grouping.txt
```

ğŸ§ªÂ **Test your code**
```bash
make test_grouping_by_key
```
<br>

ğŸ‘‰ Let's Head to data/output_intermediate_grouping.txt and have a look at the intermediary output generated <p>
ğŸ’¡Now there is only on line per key/interval. An element of the current state PCollection is a tuple
* timestamp of the interval
* List of records from all sensors

The Next step now is to reduce each element into one single value containing all the infos within a Dictionary

'''python
{
  'Timestamp': '2022-11-25 09:00:15',
  'Airtight': 1216.66,
  'AtmP': 991.14,
  'H2OC': 9.77,
  'Temp': 8.85
}
'''

ğŸ‘‰Let's map a function that is going to output a dictionary containing the below schema. Let's call this transformation `Average within interval` and implement the function `aggregate_sensors`


<details>
    <summary markdown='span'>ğŸ’¡ Hints</summary>

You can use pandas as seen in the first challenge with `pandas.pivot_table`
</details>
<br>


You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output.txt
```

ğŸ§ªÂ **Test your code**
```bash
make test_beam_final
```
<br>
<br>

# 6ï¸âƒ£ Let's Load the data into BigQuery


**ğŸ’»  Create a dataset in BigQuery and add 1 new _tables_, `stream_test`, to the dataset.
Under schema click on Edit as text and paste the following schema :

```python
Timestamp:TIMESTAMP, Airtight:FLOAT, AtmP:FLOAT, H2OC:FLOAT, Temp:FLOAT
```

**ğŸ“ Fill in the `BQ_TABLE` variable in the `.env` project configuration**
! don't forget to put your project id 


# ğŸConclusion
CongratulationğŸ‰ Tou have build a complete pipeline to transform and load your data <br>
Here is an overview of what you have build during this challenge
<img src="https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/0304-02-graphviz-pipeline.svg" height=500>



# (Bonus) Time Window Session
<br>

modify pipeline windowing
create beam.window.TimestampedValue
apply beam.WindowInto
apply a MapSessionWindow by getting the window end time within the function process of the class
