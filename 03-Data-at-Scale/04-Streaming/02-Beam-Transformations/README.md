# Objectives
<br>

🎯 The goal is to:
* Group data coming from different IoT sources within a fixed interval of time
* Load the transformed data into **BigQuery**

✅ Before Thinking about streaming the data, we have provided you an old log file which contains values from different sensors with the timestamp. `Streaming` will be covered in next challenge 🙌


# 1️⃣ Download the data

Let's first download the data that we are going to work with during this challenge.
To do so, let's execute this command:
```bash
mkdir -p data
curl https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/sensors_latency.csv > data/sensors_latency.csv
```
👉 Let's have a quick look at the data in the terminal
```bash
head data/sensors_latency.csv
```
☝️ The data contains 3 pieces of information:
- The `sensor` name
- The `value` recorded
- The `timeStamp` corresponding to the time of the record
<p>💪 Let's build our ETL pipeline with Apache Beam to transform and load this data in BigQuery</p>

# 2️⃣ Overview of the Pipeline

During this challenge, We are going to use the paradigm of Map/Reduce with Apache Beam and build step-by-step our pipeline<br>
👇 Here is an overview of this pipeline

<img src="https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/03-04-02_beam_pipeline.png" witdh=400>


👉 Head to `app/beam_sample.py` and let's have a look at the different part of the pipeline within the code inside the function `run`

- First a beam Pipeline is instantiated as `pipeline`
- `data`: is the part of the pipeline where data will be loaded
- `windowing`: take as input `data` and will prepare the data to be grouped
- `grouping`: take as input `windowing` then will group and transform the data by key
- `out`: write the output of the pipeline

# 3️⃣ Read the data with Beam 🔦

❓ Let's Code the first part of the pipeline `data` in `app/beam_sample.py`:
- `ReadData` method that will read a text file with the `file_path`variable
- `Convert to list` method that will convert each element into a list

<br>

💡 To Debug your code you can map a function to print each element of a PCollection :
```python
data | 'Print in Terminal' >> beam.Map(lambda x: print(x))
```
☝️ Notice the syntax of Beam to Apply a method with the pipe ( **|** ) and name it with ( **>>** ).

Now you can execute your code with this command:
```bash
python app/beam_sample.py --input data/sensors_latency.csv
```

You should be able to see your data being printed!

# 4️⃣ Save Data to an outputFile 💾

Next step is to save the transformed data in an output File.<br>
👉Head to the `out` pipeline section and save the data using `output_fps_prefix` variable
<br>

💡 You can apply the method to `grouping` PCollection (For the moment `windowing` and `grouping` are equal to `data` PCollection and they will be implement in the next steps)

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_first_read.txt
```

🧪 **Test your code**
```bash
make test_first_read
```

# 5️⃣ Let's Create a key for each element
<br>

Inside `windowing` transformation, Let's create a (key,value) Tuple so it will be possible to group the elements by keys later.

❓ What could be a good way to group the data for this type of data?
<details>
    <summary markdown='span'>💡 Hints</summary>

for a TimeStamped data, a good strategy of grouping is to create intervals to gather data from the same time window!

</details>
<br>

👉 Let's code the process by applying a Map method that we name `Map Time Rounded Key per Element`to create a Tuple elements with :
* key: rounded value of the timestamp (You can use `strtime_window_rounded` function provided for you)
* value: sensor name and sensor value


❗️ We have manually created the time intervals by rounding the timestamp. Actually Beam is able to create time windows with TimeStamped elements -> Let's leave the next exercise.


You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_intermediate_windowing.txt
```

👉 Let's Head to data/output_intermediate_windowing.txt and have a look at the intermediary output generated by implement windowing transformations.

💡 Each line is tuple the first element being `timestamps` in 15 second intervals. the second element is a list containing the sensor name and value!


🧪 **Test your code**
```bash
make test_windowing
```
With this tuple we are now ready to group!

# 6️⃣ Let's Group/Combine the data
<br>

❓ In The previous step, we already created a key for each element.
So now, we just need to group these elements together. Let's call this transformation `Group By Key` and implement it as the first transformation in the `windowing` section!

<details>
    <summary markdown='span'>💡 Hints</summary>
You can have a look at the Apache Beam `GroupByKey` Documentation <a href="https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey/">here</a>
</details>
<br>

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_intermediate_grouping.txt
```

🧪 **Test your code**
```bash
make test_grouping_by_key
```
<br>

👉 Let's Head to data/output_intermediate_grouping.txt and have a look at the intermediary output generated <p>
💡Now there is only one element per 15 second interval. Currently the tuple contains:
* a timestamp of the interval
* a list of records from all sensors during that window

The next step now is to reduce that long list into one single value containing all the info within a Dictionary:

```python
{
  'Timestamp': '2022-11-25 09:00:15',
  'Airtight': 1216.66,
  'AtmP': 991.14,
  'H2OC': 9.77,
  'Temp': 8.85
}
```

❓ Let's map a function `aggregate_sensors` that is going to output a dictionary containing the below schema from that list. Then apply it is within the `windowing` section as the transformation `Average Over Time`.


<details>
    <summary markdown='span'>💡 Hints</summary>

You can use pandas as seen in the first challenge with `pandas.pivot_table`.
</details>

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output.txt
```

🧪 **Test your code**
```bash
make test_beam_final
```
<br>
<br>

# 6️⃣ Let's Load the data into BigQuery


❓ 💻  Create a dataset in BigQuery and add 1 new _tables_, `stream-test`, to the dataset.
Under schema click on Edit as text and paste the following schema :

```python
Timestamp:TIMESTAMP, Airtight:FLOAT, AtmP:FLOAT, H2OC:FLOAT, Temp:FLOAT
```

**📝 Fill in the `BQ_TABLE` variable in the `.env` project configuration**
with the right format:
```bash
BQ_TABLE=<your-project-id>:<your-bq-dataset-name>.<your-table-name>
```

👉 Let's head to `out_bq` part of the pipeline and add `Write to Big Query` transformation using
[beam.io.gcp.WriteToBigQuery](https://beam.apache.org/releases/pydoc/2.8.0/apache_beam.io.gcp.bigquery.html)
<br>

❗ A temp location folder in google cloud storage is mandatory to load data into **BigQuery** when writing all of the data at once we won't need one for streaming. You can create a folder in your bucket and store the path in `GCS_TEMP`in your `.env`

You can execute your code with this command by enabling the last part of the pipeline:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output.txt \
    --out-bq True
```

👉 Let's head to your gcp console into **BigQuery** and check that the data has loaded in the format you expect!


# 🏁 Conclusion

Congratulations 🎉 Tou have built a complete pipeline to transform and load your data. Here is an overview of what you have built during this challenge:

<img src="https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/0304-02-graphviz-pipeline.svg" height=500>
