# Objectives
<br>

🎯 The goal is to:
* group data coming from different IoT within a fix interval of time
* load the transformed data in BigQuery

✅ Before Thinking about streaming the data, we have provided you an old log file which contains values from different sensors with the timestamp. `Streaming` will be covered in next challenge 🙌

<br>
<br>

# 1️⃣ Download the data
<br>

Let's first download the data that we are going to work with during this challenge.
To do so, let's execute this command:
```bash
mkdir -p data
curl https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/sensors_latency.csv > data/sensors_latency.csv
```
👉 Let's have a quick look to data in the terminal
```bash
head data/sensors_latency.csv
```
☝️ The data contains 3 informations:
- The `sensor` name
- The `value` recorded
- The `timeStamp` corresponding to the time of the record
<p>💪 Let's build our ETL pipeline with Apache Beam to transform and load this data in BigQuery</p>
<br>
<br>

# 2️⃣ Overview of the Pipeline
<br>

During this challenge, We are going to use the paradigm of Map/Reduce with Apache Beam and build step-by-step our pipeline<br>
👇 Here is an overview of this pipeline

<img src="https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/03-04-02_beam_pipeline.png" witdh=400>


👉 Head to `app/beam_sample.py` and let's have a look at the different part of the pipeline within the code inside the function `run`

- First a beam Pipeline is instantiated as `pipeline`
- `data`: is the part of the pipeline where data will be loaded
- `windowing`: take as input `data` and will prepare the data to be grouped
- `grouping`: take as input `windowing` then will group and transform the data by key
- `out`: write the output of the pipeline


<br>
<br>

# 3️⃣ Read the data with Beam
<br>

👉 Let's Code the first part of the pipeline in `data` :
- `ReadData` method that will read a text file with the `file_path`variable
- `Convert to list` method that will convert each element into a list


💡 To Debug your code you can map a function to print each element of a PCollection :
```python
data | 'Print in Terminal' >> beam.Map(lambda x: print(x))
```
☝️ Notice the syntax of Beam to Apply a method with the pipe (|) and name it with >>
<br>
<br>
Now you can execute your code with this command:
```bash
python app/beam_sample.py --input data/sensors_latency.csv
```
<br>
<br>

# 4️⃣ Save Data to an outputFile
<br>

Next step is to save the transformed data in an output File.<br>
👉Head to `out` pipeline and save the data using `output_fps_prefix` variable
<br>

💡 You can apply the method to `grouping` PCollection (For the moment `windowing` and `grouping` are equal to `data` PCollection and they will be implement in the next steps)

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_first_read.txt
```

🧪 **Test your code**
```bash
make test_first_read
```
<br>
<br>

# 5️⃣ Let's Create a key for each element
<br>

Inside `windowing` transformation, Let's create a (key,value) Tuple so it will be possible to group the elements by keys later. <br>
❓In your opinion, what could be a good way to group the data
<details>
    <summary markdown='span'>💡 Hints</summary>

for a TimeStamped data, a good strategy of grouping is to create intervals to gather data from the same time window
</details>
<br>

👉 Let's code the process by applying a Map method that we name `Map Time Rounded Key per Element`to create a Tuple elements with :
* key: rounded value of the timestamp (You can use `strtime_window_rounded` function)
* value: sensor name and sensor value

❗️We have manually created the time intervals by rounding the timestamp. Actually Beam is able to created time windows with TimeStamped elements -> Let's leave that for the BONUS part.


You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_intermediate_windowing.txt
```

👉 Let's Head to data/output_intermediate_windowing.txt and have a look at the intermediary output generated by implement windowing transformations.<p>
💡All the first line have the same key with the rounded time, the data is now ready to be grouped into different intervals


🧪 **Test your code**
```bash
make test_windowing
```
<br>
<br>

# 6️⃣ Let's Group/Combine the data
<br>

First we need to group the data. In The previous step, we have already created a key for each element.
So now, we just need to group these elements toghether. Let's call this transformation `Group By Key`

<details>
    <summary markdown='span'>💡 Hints</summary>
You can have a look at the Apache Beam `GroupByKey` Documentation <a href="https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey/">here</a>
</details>
<br>

You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output_intermediate_grouping.txt
```

🧪 **Test your code**
```bash
make test_grouping_by_key
```
<br>

👉 Let's Head to data/output_intermediate_grouping.txt and have a look at the intermediary output generated <p>
💡Now there is only on line per key/interval. An element of the current state PCollection is a tuple
* timestamp of the interval
* List of records from all sensors

The Next step now is to reduce each element into one single value containing all the infos within a Dictionary

'''python
{
  'Timestamp': '2022-11-25 09:00:15',
  'Airtight': 1216.66,
  'AtmP': 991.14,
  'H2OC': 9.77,
  'Temp': 8.85
}
'''

👉Let's map a function that is going to output a dictionary containing the below schema. Let's call this transformation `Average within interval` and implement the function `aggregate_sensors`


<details>
    <summary markdown='span'>💡 Hints</summary>

You can use pandas as seen in the first challenge with `pandas.pivot_table`
</details>
<br>


You can execute your code with this command:
```bash
python app/beam_sample.py \
    --input data/sensors_latency.csv \
    --output data/output.txt
```

🧪 **Test your code**
```bash
make test_beam_final
```
<br>
<br>

# 6️⃣ Let's Load the data into BigQuery


**💻  Create a dataset in BigQuery and add 1 new _tables_, `stream_test`, to the dataset.
Under schema click on Edit as text and paste the following schema :

```python
Timestamp:TIMESTAMP, Airtight:FLOAT, AtmP:FLOAT, H2OC:FLOAT, Temp:FLOAT
```

**📝 Fill in the `BQ_TABLE` variable in the `.env` project configuration**
! don't forget to put your project id 


# 🏁Conclusion
Congratulation🎉 Tou have build a complete pipeline to transform and load your data <br>
Here is an overview of what you have build during this challenge
<img src="https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D4/0304-02-graphviz-pipeline.svg" height=500>



# (Bonus) Time Window Session
<br>

modify pipeline windowing
create beam.window.TimestampedValue
apply beam.WindowInto
apply a MapSessionWindow by getting the window end time within the function process of the class
