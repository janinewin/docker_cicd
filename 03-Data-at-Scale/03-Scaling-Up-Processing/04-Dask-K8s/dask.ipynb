{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K8s Dask notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Import and access the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import HelmCluster\n",
    "cluster = HelmCluster(release_name=\"mydask\")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Look at how much RAM and Thread you have access to! üèãÔ∏è‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Plug the cluster to the client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below üëá to add the **cluster** to the client. You will see some warnings about mismatched versions ‚ö†Ô∏è. This is okay provided that it is not a library we are trying to use when we distribute our code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Testing our cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even scale the number of worker pods from inside dask!\n",
    "\n",
    "Let's start by scaling the cluster down to **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this random super-heavy computation on dataframe of size **2.5 billion** and check your Status on Dask dashboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.array as da\n",
    "\n",
    "axis_size = 50_000\n",
    "chunk = 5_000\n",
    "\n",
    "x = da.random.random((axis_size, axis_size), chunks=(chunk, chunk))\n",
    "y = x + x.T\n",
    "z = y[::2, axis_size/2:].mean(axis=1)\n",
    "\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è You should see 3 thread running in parallel on your dashboard !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's increase it to its max level (6), then re-run cell above!\n",
    "cluster.scale(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è 18 thread should run in parallel to accelerate this CPU-bound task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Utilising our power for real use caseüí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Our goal is to get all of the summary statistics (means, std...) from the yellow taxi dataset, for the two years `2020-2021`. \n",
    "\n",
    "This real dataset comprises of `60` million rows, each with `19` columns!\n",
    "\n",
    "It is split in `24` monthly parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) One file / one month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "description = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(description, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) 24 months at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see how long it takes us to access one of the **twenty four** parquet files we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1) Pure pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code we used for **naive** pandas version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_statistics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for i in range(1,13):\n",
    "    if i < 10:\n",
    "        i = f\"0{i}\"\n",
    "    print(f\"downloading 2020-{i}...\")\n",
    "    df = pd.read_parquet(f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-{i}.parquet\")\n",
    "    monthly_statistics.append(df.describe())\n",
    "    print(f\"downloading 2021-{i}...\")\n",
    "    df = pd.read_parquet(f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-{i}.parquet\")\n",
    "    monthly_statistics.append(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We computed our 24 statistics in a single thread...but it took ages!\n",
    "len(monthly_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing 2-year averages is now very quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_means = sum([statistic.loc[\"mean\",:] for statistic in monthly_statistics])/len(monthly_statistics)\n",
    "global_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3) DASK üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we decorate the functions we want to distrbute with `@dask.delayed`, here we have created two seperate tasks to help you distinguish them on the dashboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def twenty_twenty_monthly_describe(i: int):\n",
    "    if i < 10:\n",
    "        i = f\"0{i}\"\n",
    "    df = pd.read_parquet(f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-{i}.parquet\")\n",
    "    return df.describe()\n",
    "\n",
    "@dask.delayed\n",
    "def twenty_twenty_one_monthly_describe(i: int):\n",
    "    if i < 10:\n",
    "        i = f\"0{i}\"\n",
    "    df = pd.read_parquet(f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-{i}.parquet\")\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a similar list of 24 monthly description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = []\n",
    "\n",
    "for i in range(1,13):\n",
    "    monthly_data.append(twenty_twenty_monthly_describe(i))\n",
    "    monthly_data.append(twenty_twenty_one_monthly_describe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è They are all these delayed objects, **dask** is lazy until we call **compute()** and then works out the result of these objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Run this and watch the dashboard and see dask tear through this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "monthly_statistics = dask.compute(monthly_data)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(monthly_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_means = sum([statistic.loc[\"mean\",:] for statistic in monthly_statistics])/len(monthly_statistics)\n",
    "global_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Dig inside one specific task profile, and try to figure out the share of time spend on I/O tasks (downloading parquets) vs. CPU tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W3D3-processing/dask/task-dashboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö® If you want to delete your cluster now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "gcloud container clusters delete mydaskcluster --zone=europe-west-1b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otherwise, you can play with other Dask-made introduction notebooks by following step 4Ô∏è‚É£ in the README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
