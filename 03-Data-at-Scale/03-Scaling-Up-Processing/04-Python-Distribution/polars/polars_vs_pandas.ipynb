{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4735a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857eab9",
   "metadata": {},
   "source": [
    "‚ùì Count the number of cpu's using the OS module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2b2c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of cpu's \n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6da08b",
   "metadata": {},
   "source": [
    "üí° Pandas only uses one of these CPU's, while Polars ia able to use all of them for **parallel processing**. Its ideal use case is data too big for pandas and too small for spark. Polars offers both an eager and a **lazy API**. The lazy API is said to be ‚Äòsomewhat similar to spark‚Äô. The lazy API allows the user to optimise the query before it runs, promising ‚Äòblazingly fast‚Äô performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b34fedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of random numbers\n",
    "values = [random.random() for _ in range(100000)]\n",
    "\n",
    "# Convert the list to a pandas DataFrame and a polars DataFrame\n",
    "df_pandas = pd.DataFrame(values)\n",
    "df_polars = pl.DataFrame(values)\n",
    "\n",
    "# Compute the mean of the values in the pandas DataFrame\n",
    "start_time = time.time()\n",
    "mean_pandas = df_pandas.mean()\n",
    "elapsed_time = time.time() - start_time \n",
    "print(f\"Mean computation with pandas took {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Compute the mean of the values in the polars DataFrame\n",
    "start_time = time.time()\n",
    "mean_polars = df_polars.mean()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Mean computation with polars took {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e090f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of the values in the pandas DataFrame\n",
    "start_time = time.time()\n",
    "mean_pandas = df_pandas.std()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Std computation with pandas took {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Compute the mean of the values in the polars DataFrame\n",
    "start_time = time.time()\n",
    "mean_polars = df_polars.std()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Std computation with polars took {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb58b6a",
   "metadata": {},
   "source": [
    "### Lets load a csv and see what lazy evaluation means\n",
    "\n",
    "We are using the pandas library to first load the data from S3, storing it as a CSV, and then loading it using Polarsin the lazy modus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e851945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('s3://wagon-public-datasets/data-engineering/W3D3-processing/data/winemag-data_first150k.csv')\n",
    "df.to_csv('winemag-data_first150k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea9234cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df = pl.scan_csv('winemag-data_first150k.csv', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fea1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd0ae0",
   "metadata": {},
   "source": [
    "ü§î Wait, the data is not loaded yet? What if we do some transformations on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b691fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966dff5",
   "metadata": {},
   "source": [
    "üí° As we can see nothing happens right away. From the documentation: ‚ÄòThis is due to the lazyness, nothing will happen until specifically requested. This allows Polars to see the whole context of a query and optimize just in time for execution.‚Äô Lets see the optimized version! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e801b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ").show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9d476",
   "metadata": {},
   "source": [
    "To actually see the results we can do two things: \n",
    "\n",
    "1Ô∏è‚É£ **collect()** ‚Üí runs the query over all the results\n",
    "\n",
    "2Ô∏è‚É£ **fetch()** ‚Üí takes the first 500 rows and runs the query\n",
    "\n",
    "Fetch takes the first 500 rows (or less) and runs the query. Collect runs the query over all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce82f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pl.scan_csv('winemag-data_first150k.csv', ignore_errors=True).filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ").fetch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbece5",
   "metadata": {},
   "source": [
    "ü§î What if we want to load the data from the csv file using Pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36ed915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.read_csv(\"winemag-data_first150k.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e183157",
   "metadata": {},
   "source": [
    "ü§Ø It is faster to load and filter the data using Polarsthan it is to load the first few rows with Pandas without any filtering. Parallel processing and optimizing queries can bring big advantages in terms of computational speed. Lets continue to Pyspark to further explore these concepts! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe7585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d70808e39c2f62217b6a024347d74910f9925df71aa2cfbc78e4d1d98fb4f4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
