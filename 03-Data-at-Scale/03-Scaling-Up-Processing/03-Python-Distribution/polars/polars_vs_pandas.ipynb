{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Polar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newest and potentially fastest in the gang of parallel processing dataframes !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polars offers both an eager and a **lazy API**. The lazy API allows the user to optimise the query before it runs, promising `blazingly fast` performance.\n",
    "- Polar is coded in Rust, while Spark is coded in Scala, and Dask is coded in C++ (Dvia Pandas/Numpy)\n",
    "- Under the hood, Dask breaks a single large data processing job into many smaller tasks, which are then handled by numpy or pandas. \n",
    "\n",
    "\n",
    "[Polar vs. Pandas](https://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html)\n",
    "\n",
    "[Polar vs. Spark](https://pola-rs.github.io/polars-book/user-guide/coming_from_spark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! apt install graphviz to see the computational DAG graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Count the number of cpu's using the OS module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of cpu's \n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of random numbers\n",
    "values = [random.random() for _ in range(100000)]\n",
    "\n",
    "# Convert the list to a pandas DataFrame and a polars DataFrame\n",
    "df_pandas = pd.DataFrame(values)\n",
    "df_polars = pl.DataFrame(values)\n",
    "\n",
    "# Compute the mean of the values in the pandas DataFrame\n",
    "start_time = time.time()\n",
    "mean_pandas = df_pandas.mean()\n",
    "elapsed_time = time.time() - start_time \n",
    "print(f\"Mean computation with pandas took {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Compute the mean of the values in the polars DataFrame\n",
    "start_time = time.time()\n",
    "mean_polars = df_polars.mean()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Mean computation with polars took {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of the values in the pandas DataFrame\n",
    "start_time = time.time()\n",
    "mean_pandas = df_pandas.std()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Std computation with pandas took {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Compute the mean of the values in the polars DataFrame\n",
    "start_time = time.time()\n",
    "mean_polars = df_polars.std()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Std computation with polars took {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets load a csv and see what lazy evaluation means\n",
    "\n",
    "We are using the pandas library to first load the data from S3, storing it as a CSV, and then loading it using Polarsin the lazy modus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('s3://wagon-public-datasets/data-engineering/W3D3-processing/data/winemag-data_first150k.csv')\n",
    "df.to_csv('winemag-data_first150k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df = pl.scan_csv('winemag-data_first150k.csv', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Wait, the data is not loaded yet? What if we do some transformations on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As we can see nothing happens right away. From the documentation: ‚ÄòThis is due to the lazyness, nothing will happen until specifically requested. This allows Polars to see the whole context of a query and optimize just in time for execution.‚Äô Lets see the optimized version! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ").show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually see the results we can do two things: \n",
    "\n",
    "1Ô∏è‚É£ **collect()** ‚Üí runs the query over all the results\n",
    "\n",
    "2Ô∏è‚É£ **fetch()** ‚Üí takes the first 500 rows and runs the query\n",
    "\n",
    "Fetch takes the first 500 rows (or less) and runs the query. Collect runs the query over all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pl.scan_csv('winemag-data_first150k.csv', ignore_errors=True).filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ").fetch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î What if we want to load the data from the csv file using Pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.read_csv(\"winemag-data_first150k.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§Ø It is faster to load and filter the data using Polarsthan it is to load the first few rows with Pandas without any filtering. Parallel processing and optimizing queries can bring big advantages in terms of computational speed. Lets continue to Pyspark to further explore these concepts! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
