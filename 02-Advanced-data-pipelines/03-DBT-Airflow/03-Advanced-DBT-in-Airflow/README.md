# Track long

### Introduction

You already integrated dbt to Airflow at a project level. In this exercise, your will integrate it at a model level. This means that your goal will be to have a DAG containing one task per model (and not just two tasks as before).

To complete this task you will base your DAG on the (`manifest.json`)[https://docs.getdbt.com/reference/artifacts/manifest-json] generated by dbt. As you can see by reading the documentation and opening the `manifest.json` that we prepared for you in your `lewagon_dbt` folder, this file contains all models and their dependencies.

For this exercise, you will mainly work on the `dbt_advanced.py` file as we already prepared most of the setup. You just have to recreate the same `profiles.yml` and `.env` as for the previous exercise (add `_optional` at the end of your dataset's name, such that you can distinguish it).

PS: We removed most of the `manifest.json` content to keep only the needed part for you and make it more readable, but usually, this file contains thousands of lines.

## Setup the DAG

Open the `dbt_advanced.py` file and add your DAG inside. It should:
- be named `dbt_advanced`
- depend on past
- not catchup
- start from yesterday and run daily

For the first time, you won't manually declare your task but you will generate them by using python functions. To help you understand this new concepts, we already added for you the functions calls such that you just have to code the functions themselves.

## Setup the python functions

As just explained, we have prepared the functions signatures and the functions calls that you will have to use. Each function also contains a description of what it is supposed to do. However, to make sure you fully understand the whole process, here is a summary:
- the `load_manifest` function will be called first and will be given the `manifest.json` path that it will load as a `dict` and return
- the returned `dict` will be given to the `create_tasks` function that will build a dict containing the `nodes` of the `manifest.json` as keys and their corresponding BashOperators
- these BashOperators will be built by the `make_dbt_task` function that will be called by the `create_tasks` function for each node (for which the associated dbt verb should be given too: `run` or `test`)
- the `create_dags_dependencies` function will reuse this `dict` to create the Airflow tasks. To order them properly, this function will have to manipulate the `depends_on` field of the `manifest.json`

This exercise is an optional and will be quite challenging to implement, so do not hesitate to check with a teacher that you have properly understood the requirements before starting.

At the end, you should have a DAG that looks like [this](https://wagon-public-datasets.s3.amazonaws.com/data-engineering/W2D3/dbt_dag.png).

Again, once done, do not hesitate to make it work with your own models as well.
