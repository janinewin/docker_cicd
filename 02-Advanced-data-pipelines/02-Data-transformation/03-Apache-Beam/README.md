# Map Reduce -> Spark -> Apache Beam

## Really basic Python

- Use the [stdlib](https://book.pythontips.com/en/latest/map_filter.html)

## Implement a basic Map-Reduce purely in Python

- Inspiration from [this tutorial](https://nyu-cds.github.io/python-bigdata/02-mapreduce/)

## Use Pyspark

- Inspiration from [this tutorial](https://nyu-cds.github.io/python-bigdata/03-spark/)

## Use Apache Beam

- Count words [example](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py)

## Beam on Google Cloud

- Transform the Beam code to run on [Google Dataflow](https://github.com/tuanavu/google-dataflow-examples/blob/master/examples/wordcount.py)
