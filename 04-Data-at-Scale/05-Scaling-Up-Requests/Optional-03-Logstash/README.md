### ğŸ¯ Goal

Kudos on getting to the optional challenge! ğŸ™Œ

This morning we saw [Logstash](https://www.elastic.co/guide/en/logstash/7.17/introduction.html) - a tool from the Elasticstack that can be used to ingest, transform and output log data. We used it to stash logs from one file to another, and in this exercise you will take that setup to the next level by **connecting Logstash to multiple microservices on Docker Compose!** Let's get started! ğŸš€

# 1ï¸âƒ£ Starter Setup âš™ï¸

Start with exploring the following files and folders:

- `api/`
- `database/`
- `dockerfile-fastapi`
- `docker-compose.yml`

You'll see a familiar setup with a FastAPI app connected to a PostgreSQL database!

â“ Go ahead and `build` and `up` your Docker Compose stack. Check the exposed FastAPI port in the browser, you should see:

```json
{ "running": "true" }
```

In `main.py` or the autogenerated `/docs` endpoint of your API you should have noticed another endpoint that pulls data from the database - however there's no data at the moment ğŸ•³ï¸

**Let's bring back the F1 dataset from last module ğŸï¸**

â“ Download the starter F1 dataset SQL script and unzip it with `gzip`

```bash
curl --output ./database/f1db.sql.gz https://storage.googleapis.com/lewagon-data-engineering-bootcamp-assets/datasets/f1/f1db.sql.gz

```

ğŸ¤” Ask yourself why we put the SQL script in the `database` folder. The answer lies in `docker-compose.yml`

â“ Launch the Docker Compose stack again. Is the data there? Some table names you can try are `drivers`, `circuits`, `constructors`. API response is looking good? ğŸš¥ Let's get started with Logstash!

# 2ï¸âƒ£ Logstash Container ğŸšš

We'll need to add a Logstash container to the existing `docker-compose.yml`.

Luckily, Logstash (and other Elasticstack components) offer [base Docker images](https://www.docker.elastic.co/r/logstash)!

â“ Add a `logstash` container to the Docker Compose stack with version `7.14.0` of the image on the same network as FastAPI and PostgreSQL containers

### Volumes

We'll need to write and pass a Logstash configuration file to the container. We'll probably also want to see it in action by binding the folder with the logs to our VM! ğŸ“

â“ Create a `logstash.conf` file in the challenge root and bind it to the default location for Logstash configuration files - `/usr/share/logstash/pipeline/`

â“ Then create a `logs` folder and bind it to a folder of the same name in `/usr/share/logstash/data/` inside the container

### Ports and User

This part is a little tricky, because Docker Compose container logs are being output to the Docker Daemon, not the containers themselves.

So adding the usual `port:port` wouldn't be enough to allow Logstash to read the console logs of other containers. We need to **expose the port of Logstash to the localhost of the Docker Daemon**. ğŸ”—

â“ Check out [this section](https://github.com/compose-spec/compose-spec/blob/master/spec.md#ports) of the Docker Compose docks to see how to do that and **publish TCP ports `5000` and `5001` of `logstash` to the Docker Daemon host IP.**

We need two ports as one will be listening to logs from FastAPI and the other - from PostgreSQL! ğŸ¤

Finally, as Logstash needs access to the `/usr/shared` files, we need to run the container with higher permissions.

â“ Set the user of the container to `root`

ğŸš€ Run `docker-compose up` again - you should now have an extra `logstash` starting! It will error out, as there's nothing defined in `logstash.conf` yet, but you're on the right track!

### `logstash.conf`

Time to create a basic configuration for our Logstash pipeline.

â“ Check out these [Logstash pipeline examples](https://www.elastic.co/guide/en/logstash/current/config-examples.html) and update `logstash.conf` to have:

- two TCP inputs for the previously exposed ports 5000 and 5001
- one file output to `/usr/share/logstash/data/logs/logs.json`

â“ Start up the Compose stack. You should see the Logstash pipeline running with:

```bash
[INFO ][logstash.agent   ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
```

> Disregard any errors you see about Logstash trying to connect to Elasticsearch website. ğŸ¤·

### Connecting Other Containers to Logstash

Let's start sending `webapi` and `db` container logs to `logstash`. ğŸ›°

One of the easiest ways to do that is via the `syslog`, which is one of the common outputs for logs.

â“ Check out [Docker docs on using syslog](https://docs.docker.com/config/containers/logging/syslog/) and set the both containers to use the right log driver and the right ports. (*Remember, we want to send the logs to the port exposed to the host IP in the previous step* ğŸ’¡)

By default, PostgreSQL logs are quiet. Let's change that!

â“ Add the right `command` to the `db` container that would increase verbosity of PostgreSQL logs to the maximum by following the [docs](https://www.postgresql.org/docs/current/runtime-config-logging.html).

<details>
  <summary markdown='span'>ğŸ’¡ Hint</summary>
You can pass options to the Postgres startup command with <code>postgres -c [options]</code>
</details>

</br>

Finally, we should also set up an order dependency between containers. `webapi` already depends on `db` to start, so we can add a dependency to `db` to make sure Logstash starts first so that we don't lose any info!

â“ Add a dependency on `logstash` to the `db` container. You saw that Logstash also takes a while to start-up, so we want to make sure it's **healthy** before launching `db`. Parse out [this Elasticstack docker-compose file](https://github.com/elastic/stack-docker/blob/master/docker-compose.yml) for the right healthcheck.

ğŸš€ Launch the stack again and watch the logs flow! You should see the `logs/logs.json` file created in your folder structure with logs from both FastAPI and PostgreSQL! ğŸ“’

If you look at the contents you will realize that storing logs like this is not sustainable though:

1. Mixing both container logs makes it harder to parse them and find the right info
2. This file will soon become too big for your machine to read/write effectively - it needs rotation

# Update `logstash.conf` to production level

â“ Use these Logstash [docs](https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html) to update the configuration file by:

- tagging both inputs with 'fastapi' and 'psql' respectively
- using conditions on the tags to output each container's log to its own file
- rotate the logs every hour by having a timestamp (up to the hour) in the file name

ğŸŸ¢ If you've done everything correctly, in your `logs/` you should now see two files with timestamps and their respective logs:

```bash
fastapi_logs-YYYY-MM-DD-HH.json
psql_logs-YYYY-MM-DD-HH.json
```

# Well done! ğŸ§¨

You've set up Logstash keeping with the nature of **microservices**. Keeping thorough, structured logs will be crucial for any team for tracking performance and security of their products. And now you know how to do it with Logstash!

> Remember to commit and push your code ğŸ‘‹
